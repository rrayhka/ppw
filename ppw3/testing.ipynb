{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f3592c3-3c2e-4e86-a44e-1cbac2913354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "import nltk\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ab85d4-8436-4a65-9913-ffa375b79038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akhyar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akhyar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825176e4-8740-45ea-9ca2-c10dc3ad4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = stopwords.words('indonesian')\n",
    "\n",
    "# Simpan stopwords ke file\n",
    "with open('stopwords.txt', 'w') as f:\n",
    "    for item in stop_words:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05bcbe54-1ba3-4d47-99fe-655196b3087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mengambil konten berita dari URL\n",
    "def scrape_news(url):\n",
    "    isi = []\n",
    "    judul = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        article_full = BeautifulSoup(response.content, \"html.parser\")\n",
    "        judul_artikel = article_full.find(\"h1\", class_=\"mb-4 text-32 font-extrabold\").text.strip()\n",
    "        artikel_element = article_full.find(\"div\", class_=\"detail-text\")\n",
    "        artikel_teks = [p.get_text(strip=True) for p in artikel_element.find_all(\"p\")]\n",
    "        artikel_content = \"\\n\".join(artikel_teks)\n",
    "        isi.append(artikel_content)\n",
    "        judul.append(judul_artikel)\n",
    "    return { \"isi\": isi} if isi else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a57743eb-ef7f-49d2-a7d6-5520d248729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fungsi pembersihan teks\n",
    "def cleansing(text):\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b-\\b', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]+', ' ', text)\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb189618-00ab-4514-804e-71a32d5bbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fungsi untuk menghapus stopword\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3031bf8-a913-4096-8f53-243c2554ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fungsi stemming\n",
    "def stemming(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "# Fungsi utama untuk preprocessing teks\n",
    "def preprocess_text(text):\n",
    "    clean_text = cleansing(text)\n",
    "    return clean_text  # Hanya cleansing tanpa stopword dan stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45159cb0-5ea5-442d-92c1-0fe171d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fungsi utama untuk ringkasan dan visualisasi graf\n",
    "def summarize_and_visualize(content):\n",
    "    # Tokenisasi kalimat\n",
    "    kalimat = sent_tokenize(content)\n",
    "    \n",
    "    # Preprocessing teks\n",
    "    preprocessed_text = preprocess_text(content)\n",
    "    kalimat_preprocessing = sent_tokenize(preprocessed_text)\n",
    "    \n",
    "    # TF-IDF dan cosine similarity\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(kalimat_preprocessing)\n",
    "    cossim_prep = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Analisis jaringan dengan NetworkX\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(cossim_prep)):\n",
    "        G.add_node(i)\n",
    "        for j in range(len(cossim_prep)):\n",
    "            if cossim_prep[i][j] > 0.1 and i != j:\n",
    "                G.add_edge(i, j)\n",
    "                \n",
    "    # Hitung closeness centrality dan buat ringkasan\n",
    "    closeness_scores = nx.closeness_centrality(G)\n",
    "    sorted_closeness = sorted(closeness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ringkasan = \" \".join(kalimat[node] for node, _ in sorted_closeness[:3])\n",
    "\n",
    "    # Visualisasi graf\n",
    "    pos = nx.spring_layout(G, k=2)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='b')\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='red', arrows=True)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "    plt.title(\"Graph Representation of Sentence Similarity\")\n",
    "\n",
    "    # Simpan grafik ke dalam format base64\n",
    "    img = io.BytesIO()\n",
    "    plt.savefig(img, format='png')\n",
    "    img.seek(0)\n",
    "    graph_url = base64.b64encode(img.getvalue()).decode()\n",
    "    plt.close()\n",
    "\n",
    "    return ringkasan, graph_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31967bc7-1f7b-45ec-8c7f-6deffd541434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\ttext = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text) # Menghapus https* and www*\n",
    "\ttext = re.sub(r'@[^\\s]+', ' ', text) # Menghapus username\n",
    "\ttext = re.sub(r'[\\s]+', ' ', text) # Menghapus tambahan spasi\n",
    "\ttext = re.sub(r'#([^\\s]+)', ' ', text) # Menghapus hashtags\n",
    "\ttext = re.sub(r\"[^a-zA-Z :\\.]\", \"\", text) # Menghapus tanda baca\n",
    "\ttext = re.sub(r'\\d', ' ', text) # Menghapus angka\n",
    "\ttext = text.lower()\n",
    "\ttext = text.encode('ascii','ignore').decode('utf-8') #Menghapus ASCII dan unicode\n",
    "\ttext = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "\ttext = text.replace('\\n','') #Menghapus baris baru\n",
    "\ttext = text.strip()\n",
    "\treturn text\n",
    "\n",
    "def clean_stopword(tokens):\n",
    "\tlistStopword =  set(stopwords.words('indonesian'))\n",
    "\tfiltered_words = [word for word in tokens if word.lower() not in listStopword]\n",
    "\treturn filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3b08015-06ad-4a2c-982d-bf186fc6368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(content):\n",
    "\tresult = {}\n",
    "\tfor i, text in enumerate(tqdm(content)):\n",
    "\t\tcleaned_text = clean_text(text)\n",
    "\t\ttokens = word_tokenize(cleaned_text)\n",
    "\t\tcleaned_stopword = clean_stopword(tokens)\n",
    "\t\tresult[i] = ' '.join(cleaned_stopword)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24bd647c-7b00-45b0-a762-1afd2f485a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapped = scrape_news(\"https://www.cnbcindonesia.com/news/20241119114410-4-589340/peternak-nangis-susu-impor-bebas-pajak-kemendag-janji-buka-opsi-ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70a13f08-deec-4271-be91-163db56f2391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 173.53it/s]\n"
     ]
    }
   ],
   "source": [
    "prepos = preprocess_text(scrapped[\"isi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8774acb6-1547-486d-b45a-8720a58af37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'jakarta cnbc indonesia kebijakan pembebasan bea masuk produk susu impor australia selandia new zealand disebutsebut biang kerok harga susu produksi peternak negeri turun . peternak berharap pemerintah mengubah perundingan dagang berjalan aseanaustralianew zealand free trade area aanzfta . menanggapi kepala badan kebijakan perdagangan bk perdag kementerian perdagangan kemendag fajarini puntodewi membuka peluang mereview meninjau ulang perjanjian perdagangan bebas free trade agreementfta negara . punto fta pengkajian ulang sesuai jadwal evaluasi disepakati . fta kitab suci diubah . jepang masanya direview masanya . sekian direview contohnya jepang punto ditemui hotel borobudur jakarta selasa . evaluasi fta salah langkah diambil kebijakan perdagangan merugikan peternak lokal menjaga keseimbangan keterbukaan pasar perlindungan sektor domestik . peternak sapi perah jawa timur jawa protes mandi susu membuang susu perah akibat terserap industri pengolahan susu ips . pemicu marahnya peternak susu produksi lokal kalah saing susu impor australia selandia dibebaskan pajak bea masuknya . sisi regulasi peraturan menteri keuangan pmk nomor jenis susu bea masuknya dibebaskan . menteri koperasi menkop budi arie setiadi nasib malang peternak sapi perah rakyat indonesia disebabkan ketidakmampuan bersaing pasar negeri sejalan perjanjian perdagangan bebas indonesia selandia australia . menurutnya perjanjian perdagangan bebas indonesia selandia australia produk susu impor negara bebas bea masuk harganya murah dibandingkan produk susu negara . faktor harga hubungan kedekatan negara indonesia harga produk susu kompetitif . selandia australia memanfaatkan perjanjian perdagangan bebas indonesia menghapuskan bea masuk produk susu . harga produk rendah dibandingkan harga pengekspor produk susu global budi konferensi pers kantornya senin .'}\n"
     ]
    }
   ],
   "source": [
    "print(prepos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30f26b5f-1e93-4246-a220-6acd698326c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_visualize(content):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_preprocessing = tfidf_vectorizer.fit_transform(content)\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_preprocessing = pd.DataFrame(data=tfidf_preprocessing.toarray(), columns=terms)\n",
    "    cossim_prep = cosine_similarity(tfidf_preprocessing, tfidf_preprocessing)\n",
    "    similarity_matrix = pd.DataFrame(cossim_prep, \n",
    "                                     index=range(len(content)), \n",
    "                                     columns=range(len(content)))\n",
    "    G_preprocessing = nx.DiGraph()\n",
    "    for i in range(len(cossim_prep)):\n",
    "        G_preprocessing.add_node(i)\n",
    "    \n",
    "    for i in range(len(cossim_prep)):\n",
    "        for j in range(len(cossim_prep)):\n",
    "            similarity_preprocessing = cossim_prep[i][j]\n",
    "            if similarity_preprocessing > 0.1 and i != j:\n",
    "                G_preprocessing.add_edge(i, j)\n",
    "    \n",
    "    pos = nx.spring_layout(G_preprocessing, k=2)\n",
    "    closeness_preprocessing = nx.closeness_centrality(G_preprocessing)\n",
    "    sorted_closeness_preprocessing = sorted(closeness_preprocessing.items(), key=lambda x: x[1], reverse=True)\n",
    "    ringkasan_closeness_preprocessing = \"\"\n",
    "    print(\"Tiga Node Tertinggi Closeness Centrality Menggunakan Preprocessing:\")\n",
    "    for node, closeness_preprocessing in sorted_closeness_preprocessing[:3]:\n",
    "        top_sentence = kalimat[node]\n",
    "        ringkasan_closeness_preprocessing += top_sentence + \" \"\n",
    "        print(f\"Node {node}: Closeness Centrality = {closeness_preprocessing:.4f}\")\n",
    "        print(f\"Kalimat: {top_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c903ccfc-0950-45ac-a13e-1036e3999b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hasil \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_and_visualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m, in \u001b[0;36msummarize_and_visualize\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_and_visualize\u001b[39m(content):\n\u001b[0;32m      2\u001b[0m     tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 3\u001b[0m     tfidf_preprocessing \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     terms \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      5\u001b[0m     tfidf_preprocessing \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mtfidf_preprocessing\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mterms)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:108\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:66\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 66\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "hasil = summarize_and_visualize(prepos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a5287-4ea2-4730-851d-716596be5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Route untuk halaman utama\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    ringkasan = None\n",
    "    graph_url = None\n",
    "    artikel = None\n",
    "    if request.method == \"POST\":\n",
    "        url_input = request.form.get(\"url\")\n",
    "        if url_input:\n",
    "            # Scraping konten artikel\n",
    "            artikel = scrape_news(url_input)\n",
    "            if artikel:\n",
    "                # Analisis dan ringkasan\n",
    "                print(\" \".join(artikel[\"isi\"]))\n",
    "                ringkasan, graph_url = summarize_and_visualize(\" \".join(artikel[\"isi\"]))\n",
    "            else:\n",
    "                ringkasan = \"Gagal mengambil konten artikel.\"\n",
    "\n",
    "    return render_template(\"summary.html\", artikel=artikel, ringkasan=ringkasan, graph_url=graph_url)\n",
    "\n",
    "# Menjalankan server Flask\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
